datasets:
  marmot:
    path:
      - "datasets/marmot/table_recognition/data/english/positive"
      #- "datasets/marmot/table_recognition/data/english/negative"
      #- "datasets/marmot/table_recognition/data/chinese/positive"
      #- "datasets/marmot/table_recognition/data/chinese/negative"

train_dataset: "marmot"
val_dataset: "marmot"
# Use a small slice of data to debug
dummy: True
# How much data to use for training
train_split: !!float 0.8
# Number of training epochs
epochs: 10
# How often to print and log metrics
log_interval: 1
# Batch size
batch_size: 2
# Number of data loading workers
workers: 4
# Where should data be processed
device: "cpu"
# What ImageNet backbone to use
backbone:
  # Family of the backbone
  # Types: alexnet, densenet, inception, resnet, squeezenet, vgg
  family: "vgg"
  # Actual model to use (among the ones in the above family)
  type: "vgg16"
# What model to use for table detection
# See https://github.com/pytorch/vision/tree/master/torchvision/models/detection
model: "FasterRCNN"

# What optimizer to use for gradient descent
optimizers:
  # Type of optimizer
  type: "adam"
  # Adam optimizer
  adam:
    # Learning rate
    lr: !!float 0.001
    # L2 penalty
    weight_decay: !!float 0
    # Whether to use the AMSGrad variant of this algorithm
    amsgrad: False
  # RMSProp optimizer
  rmsprop:
    # Learning rate
    lr: !!float 0.01
    # Momentum factor
    momentum: !!float 0
    # Smoothing constant
    alpha: !!float 0.99
    # L2 penalty
    weight_decay: !!float 0
  # Stochastic Gradient Descent with momentum optimizer
  sgd:
    # Learning rate
    lr: !!float 0.001
    # Momentum factor
    momentum: !!float 0
    # L2 penalty
    weight_decay: !!float 0
    # Enables Nesterov momentum
    nesterov: False

# How to decay the learning rate during training
lr_schedulers:
  # Type of learning rate scheduler
  type: "multi_step"
  # Step scheduler
  step:
    # Period of learning rate decay
    step_size: 30
    # Multiplicative factor of learning rate decay
    gamma: !!float 0.1
    # The index of last epoch
    last_epoch: -1
  # Multi step scheduler
  multi_step:
    # List of epoch indices (must be increasing)
    milestones:
      - 30
      - 80
    # Multiplicative factor of learning rate decay
    gamma: 0.1
    # The index of last epoch
    last_epoch: -1
