datasets:
  marmot:
    path:
      - datasets/marmot/table_recognition/data/english/positive
      #- datasets/marmot/table_recognition/data/english/negative
      #- datasets/marmot/table_recognition/data/chinese/positive
      #- datasets/marmot/table_recognition/data/chinese/negative

train_dataset: marmot
val_dataset: marmot
# Random seed for all the randomness sources
random_seed: 42
# Use a small slice of data to debug
dummy:
  # Enable or disable
  enabled: True
  # Number of examples to use
  size: 10
# How much data to use for training
train_split: !!float 0.8
# Number of training epochs
epochs: 10
# How often to print and log metrics
log_interval: 1
# Batch size
batch_size: 2
# Number of data loading workers
workers: 4
# Where should data be processed
device: cpu

# What ImageNet backbone to use
backbone:
  # Family of the backbone
  # Types: alexnet, densenet, mobilenet, vgg
  family: mobilenet
  # Actual backbone to use (among the ones in the above family)
  # See https://github.com/pytorch/vision/tree/master/torchvision/models
  type: mobilenet_v2
  # If pretrained or not
  pretrained: True
  # Desired input size
  # (check backbones.py to see minimum sizes for each model)
  input_size:
    width: 224
    height: 224

# What model to use for table detection
detector:
  # Actual detector to use
  # Types: rcnn, fast_rcnn, faster_rcnn
  type: faster_rcnn
  # R-CNN configuration
  rcnn:
    # Number of maximum selective search proposals per image
    num_proposals: 2000
    # Selective search strategy
    # Types: fast, quality
    ss_type: fast
  # Fast R-CNN configuration
  fast_rcnn:
    # Number of maximum selective search proposals per image
    num_proposals: 2000
    # Selective search strategy
    # Types: fast, quality
    ss_type: fast
    # ROI pooling layer configuration
    roi_pool:
      # ROI pooling layer output size
      output_size:
        # ROI pooling layer output height
        height: 7
        # ROI pooling layer output width
        width: 7

# What optimizer to use for gradient descent
optimizers:
  # Type of optimizer
  type: adam
  # Adam optimizer
  adam:
    # Learning rate
    lr: !!float 0.001
    # L2 penalty
    weight_decay: !!float 0
    # Whether to use the AMSGrad variant of this algorithm
    amsgrad: False
  # RMSProp optimizer
  rmsprop:
    # Learning rate
    lr: !!float 0.01
    # Momentum factor
    momentum: !!float 0
    # Smoothing constant
    alpha: !!float 0.99
    # L2 penalty
    weight_decay: !!float 0
  # Stochastic Gradient Descent with momentum optimizer
  sgd:
    # Learning rate
    lr: !!float 0.001
    # Momentum factor
    momentum: !!float 0
    # L2 penalty
    weight_decay: !!float 0
    # Enables Nesterov momentum
    nesterov: False

# How to decay the learning rate during training
lr_schedulers:
  # Type of learning rate scheduler
  type: none
  # Step scheduler
  step:
    # Period of learning rate decay
    step_size: 30
    # Multiplicative factor of learning rate decay
    gamma: !!float 0.1
    # The index of last epoch
    last_epoch: -1
  # Multi step scheduler
  multi_step:
    # List of epoch indices (must be increasing)
    milestones:
      - 30
      - 80
    # Multiplicative factor of learning rate decay
    gamma: 0.1
    # The index of last epoch
    last_epoch: -1
